{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第4章 ニューラルネットワークの学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第3章で扱ったニューラルネットワークは、重み(${\\bf W}$)とバイアス(${\\bf b}$)を決めると、入力(${\\bf x}$)から出力(${\\bf y}$)が計算できます。この章で扱う学習は、多数のデータから${\\bf W}$と${\\bf b}$を計算する方法です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 データから学習する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データとして、多数の入力と出力の組みを持っていると仮定します。\n",
    "\n",
    "$({\\bf x}_1, {\\bf t}_1), ({\\bf x}_2, {\\bf t}_2), ({\\bf x}_3, {\\bf t}_3), \\cdots, ({\\bf x}_N, {\\bf t}_N)$\n",
    "\n",
    "RTBの場合は、$i$番目のBidRequestに付随するデータを${\\bf x}_i$とし、クリックしたかどうかを$t_i$とします。\n",
    "\n",
    "手書きの数字認識の問題では、画像のpixelデータを${\\bf x}_i$とし、0-9を表すデータを${\\bf t}_i$とします。\n",
    "\n",
    "入力${\\bf x}_i$が与えられたときに、ニューラルネットワークを使えば予測値${\\bf y}_i$が計算できます。\n",
    "\n",
    "可能な限り${\\bf y}_i$と${\\bf t}_i$が近くなるようなニューラルネットワークが構築できれば、様々な入力に対して適切な出力を返すことができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 損失関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "コンピュータを使って最良のパラメータの組み合わせを決定するためには、データをどれだけよく再現するかを表現する指標が必要です。\n",
    "\n",
    "もっとも単純な指標は、教師データ${\\bf t}_i$と予測値${\\bf y}_i$の差を使って次のように定義される二乗和誤差です。\n",
    "\n",
    "$$\n",
    "    E = \\frac{1}{2}\\sum_k ({\\bf y}_k-{\\bf t}_k)^2\n",
    "$$\n",
    "\n",
    "教師データ${\\bf t}_i$と予測値${\\bf y}_i$が大きく異なると二乗和誤差は大きくなり、乖離が小さいと二乗和誤差は小さくなることがわかります。\n",
    "\n",
    "二乗和誤差のように教師データ${\\bf t}_i$と予測値${\\bf y}_i$の違いを表現する関数を損失関数と呼びます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "別の損失関数として交差エントロピー誤差が使われます。\n",
    "\n",
    "$$\n",
    "    E = -\\sum_k t_k \\log y_k\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.3 数値微分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "損失関数をもっとも小さくするような重み(${\\bf W}$)が計算することで最良のニューラルネットワークが計算できます。\n",
    "\n",
    "実は関数を最小化する計算(最適化)は関数の傾きの計算、すなわち微分から決定できます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.3.1 微分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xs = np.linspace(-1, 1)\n",
    "def plot_square():    \n",
    "    ys = xs*xs\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.plot(xs, ys, \"k-\", label=\"y=f(x)\")\n",
    "plot_square()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上の図は関数$f(x)=x^2$をプロットしたものです。この関数値(縦軸の値)をもっとも小さくなるような値(横軸の値)は何でしょうか？\n",
    "\n",
    "答えは明確で$x=0$です。では、$x=0$の特徴は何でしょうか？\n",
    "\n",
    "ひとつの特徴は、$x=0$において関数のグラフの**傾き**が$x$軸に対して水平になっていることです。\n",
    "\n",
    "**傾き**をもう少し詳しく見ていきましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "直線の場合は**傾き**は明確です。直線を$y=ax$で表現すれば、$a$の値が傾きになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e63646994474b5e89eb4e809fc33eec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='a', max=3.0, min=-3.0, step=0.5), Output()), _dom_cl…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.f(a)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(a):\n",
    "    xmin = -1\n",
    "    xmax =  1\n",
    "    \n",
    "    xs = np.linspace(xmin, xmax)\n",
    "    ys = a * xs\n",
    "    plt.plot([xmin, xmax], [0, 0], \"k--\")\n",
    "    plt.plot([0, 0], [xmin, xmax],  \"k--\")\n",
    "    plt.plot(xs, ys, lw=2)\n",
    "    plt.xlim(xmin, xmax)\n",
    "    plt.ylim(-1, 1)\n",
    "    plt.xlabel(\"$x_1$\")\n",
    "    plt.ylabel(\"$x_2$\")\n",
    "\n",
    "interact(f, a=(-3, 3, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "では、曲線の傾きはどのように定義されるでしょうか。それは特定の点における接線の傾きとして定義されます。\n",
    "\n",
    "関数$f(x)$の$x=x_0$における傾きを次のような記号を使って表現します。\n",
    "\n",
    "$$\n",
    "\\frac{df}{dx}(x_0)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4693525e47934065a591db880aa24f1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='x0', max=1.0, min=-1.0, step=0.25), Output()), _dom_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.f(x0)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def plot_slope(x0, label):\n",
    "    ys = 2*x0*(xs - x0) + x0*x0\n",
    "    plt.plot(xs, ys, label=\"slope at x={}\".format(x0))\n",
    "\n",
    "def f(x0):\n",
    "    y0 = x0*x0\n",
    "    plot_square()\n",
    "    \n",
    "    xmin = -1\n",
    "    xmax =  1\n",
    "    \n",
    "    xs = np.linspace(xmin, xmax)\n",
    "    ys = 2*x0*(xs - x0) + x0*x0\n",
    "    plt.plot([xmin, xmax], [0, 0], \"k--\")\n",
    "    plt.plot([0, 0], [xmin, xmax],  \"k--\")\n",
    "    plt.plot(xs, ys, lw=2)\n",
    "    plt.plot([x0], [y0], \"bo\")\n",
    "    plt.xlim(xmin, xmax)\n",
    "    plt.ylim(-1, 1)\n",
    "    plt.xlabel(\"$x$\")\n",
    "    plt.ylabel(\"$y$\")   \n",
    "    print(r\"df/dx = {}\".format(2*x0))\n",
    "\n",
    "interact(f, x0=(-1, 1, 0.25))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.3.2 数値微分の例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記の計算では二次関数の微分の性質を使って、傾き(=微分)を計算しましたが一般の関数では正確な微分を計算することは難しいです。\n",
    "\n",
    "そこで、ここでは近似的な微分の計算手法である数値微分を導入します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{df}{dx}(x_0) \\approx \\frac{f(x0+h)-f(x0)}{h}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x0=1.0\n",
      "df/dx=2.0\n",
      "numerical diff = 2.000099999999172\n"
     ]
    }
   ],
   "source": [
    "def numerical_diff(f):\n",
    "    h = 1e-4\n",
    "    def __func__(x0):    \n",
    "        return (f(x0+h) - f(x0)) / h\n",
    "    return __func__\n",
    "\n",
    "def square(x):\n",
    "    return x*x\n",
    "\n",
    "x0 = 1.0\n",
    "print(\"x0={}\".format(square(x0)))\n",
    "print(\"df/dx={}\".format(2*x0))\n",
    "print(\"numerical diff = {}\".format(numerical_diff(square)(x0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "小さい$h$を使えば、二点の間を通る直線の傾きとして微分の近似値が計算できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6949a2fb170444ce8f4234f57e1f3523",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.31000000000000005, description='h', max=0.7, min=0.01), Output()), _…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.f(h)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs = np.linspace(0, 1)\n",
    "\n",
    "\n",
    "def f(h):\n",
    "    ys = square(xs)\n",
    "    plt.plot(xs, ys, \"k\")\n",
    "    \n",
    "    x0 = 0.3\n",
    "    x1 = x0 + h\n",
    "    y0 = square(x0)\n",
    "    y1 = square(x1)\n",
    "    dydx = (y1-y0)/(x1-x0)\n",
    "    ys = dydx * (xs-x0) + y0\n",
    "    plt.plot(xs, ys)\n",
    "    plt.plot([x0], [y0], \"bo\")\n",
    "    plt.plot([x1], [y1], \"ro\")    \n",
    "    \n",
    "    plt.plot([x0, x0], [0, 1], \"k--\")\n",
    "    plt.text(x0, -0.1, \"x0\", fontsize=15)\n",
    "\n",
    "    plt.plot([x1, x1], [0, 1], \"k--\")\n",
    "    plt.text(x1, -0.1, \"x0+h\", fontsize=15)\n",
    "    \n",
    "    plt.ylim(0, 1.0)\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "\n",
    "interact(f, h=(0.01, 0.7, 0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$h$が有限である限り、上記の数値積分は真の微分にはなりません。真の微分は、$h$を限りなく0に近づけた時の極限値として定義されます。\n",
    "\n",
    "$$\n",
    "    \\frac{df}{dx}(x_0) = \\lim_{h\\rightarrow 0} \\frac{f(x_0+h)-f(x_0)}{h}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.3 偏微分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "変数が多数ある関数の傾きを考えます。例えば、関数$f(x_1, x_2)$の$x_1$のみの傾きを偏微分と呼び、次のように表現します。\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x_1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.4 勾配法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "損失関数を最小化するような重み(${\\bf W}$)を計算することで最良のニューラルネットワークが計算できます。そして、関数の最小化は傾きが0になるような点を探索することで得られます。合わせて考えると、損失関数に対する重みの傾き(微分)が0になる点を探索すればよいことになります\n",
    "\n",
    "このような傾き０の点を探索する計算の代表例として勾配法があります。これはつぎのように計算した傾きを引き算する計算を繰り返す方法です。\n",
    "\n",
    "$$\n",
    "    x := x - \\eta\\frac{d f}{d x}\n",
    "$$\n",
    "\n",
    "ここで$\\eta$はパラメータで、この値が大きいほど一つ一つのstepが大きくなります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec0e30b291194673a7cb7a120434c92c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.21000000000000002, description='eta', max=0.5, min=0.01, step=0.05),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.f(eta)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import reduce\n",
    "\n",
    "def f(eta):\n",
    "    func = square\n",
    "    \n",
    "    xs = np.linspace(-1, 1)\n",
    "    ys = func(xs)\n",
    "    plt.plot(xs, ys, \"k\")\n",
    "\n",
    "    x_init = 0.8\n",
    "    xs = [x_init]\n",
    "    for i in range(10):\n",
    "        x0  = xs[-1]\n",
    "        dfdx = numerical_diff(func)(x0)\n",
    "        x1 = x0 - eta * dfdx\n",
    "        xs.append(x1)\n",
    "    \n",
    "    xs = np.array(xs)\n",
    "    ys = func(xs)\n",
    "    plt.plot(xs, ys, \"o\")\n",
    "    \n",
    "    y_init = func(x_init)\n",
    "    plt.plot(x_init, y_init, \"r\")\n",
    "    \n",
    "    plt.xlim(-1, 1)\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "interact(f, eta=(0.01, 0.5, 0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 4.5 学習アルゴリズムの実装\n",
    "\n",
    "ここまでに出てきた内容を復習すると、ニューラルネットワークは、適応可能な重みとバイアスがあり、この重みとバイアスを訓練データに適応するように調整することを「学習」と呼びます。\n",
    "\n",
    "ニューラルネットワークの学習は、次の 4 つの手順で行います。\n",
    "\n",
    "1. ミニバッチ\n",
    "  - 訓練データの中からランダムに一部のデータ（ミニバッチ）を選び出す。\n",
    "2. 勾配の算出\n",
    "  - ミニバッチの損失関数の値を減らすために、各重みパラメータの勾配を求める。\n",
    "3. パラメータの更新\n",
    "  - 重みパラメータを勾配方向に微小量だけ更新する。\n",
    "4. 繰り返す\n",
    "  - ステップ 1-3 を繰り返す。\n",
    "\n",
    "ミニバッチとして無作為に選ばれたデータを使用しているので、この方法は **確率的勾配降下法**（stochastic gradient descent）と呼ばれます。\n",
    "\n",
    "ここでは、手書き数字を学習する 2 層（隠れ層が 1 層）のニューラルネットワークを対象に、 MNIST データセットを使って学習を行います。\n",
    "\n",
    "## 4.5.1 2 層ニューラルネットワークのクラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 100)\n",
      "(100,)\n",
      "(100, 10)\n",
      "(10,)\n",
      "(784, 100)\n",
      "(100,)\n",
      "(100, 10)\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "# 2 層ニューラルネットワークのクラス\n",
    "class TwoLayerNet:\n",
    "\n",
    "    # 初期化処理\n",
    "    # input_size:入力層のニューロンの数, hidden_size:隠れ層のニューロンの数, output_size:出力層のニューロンの数\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "    # 認識（推論）処理\n",
    "    # x:画像データ\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "\n",
    "        return y\n",
    "\n",
    "    # 損失関数の値を求める\n",
    "    # x:画像データ, t:正解ラベル\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "\n",
    "        return cross_entropy_error(y, t)\n",
    "\n",
    "    # 認識精度を求める\n",
    "    # x:入力データ, t:教師データ\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "\n",
    "    # 重みパラメータに対する勾配を求める\n",
    "    # x:入力データ, t:教師データ\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "\n",
    "        return grads\n",
    "\n",
    "    # 重みパラメータに対する勾配を求める\n",
    "    # numerical_gradient() の高速版（詳細は次章）\n",
    "    def gradient(self, x, t):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        grads = {}\n",
    "\n",
    "        batch_num = x.shape[0]\n",
    "\n",
    "        # forward\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "\n",
    "        # backward\n",
    "        dy = (y - t) / batch_num\n",
    "        grads['W2'] = np.dot(z1.T, dy)\n",
    "        grads['b2'] = np.sum(dy, axis=0)\n",
    "\n",
    "        dz1 = np.dot(dy, W2.T)\n",
    "        da1 = sigmoid_grad(a1) * dz1\n",
    "        grads['W1'] = np.dot(x.T, da1)\n",
    "        grads['b1'] = np.sum(da1, axis=0)\n",
    "\n",
    "        return grads\n",
    "\n",
    "net = TwoLayerNet(input_size=784, hidden_size=100, output_size=10)\n",
    "print(net.params['W1'].shape) # 1 層目の重み\n",
    "print(net.params['b1'].shape) # 1 層目のバイアス\n",
    "print(net.params['W2'].shape) # 2 層目の重み\n",
    "print(net.params['b2'].shape) # 2 層目のバイアス\n",
    "\n",
    "x = np.random.rand(10, 784) # ダミーの入力データ（100 枚分）\n",
    "t = np.random.rand(10, 10)  # ダミーの正解ラベル（100 枚分）\n",
    "\n",
    "grads = net.numerical_gradient(x, t) # 勾配を計算\n",
    "\n",
    "print(grads['W1'].shape) # 1 層目の重みの勾配\n",
    "print(grads['b1'].shape) # 1 層目のバイアスの勾配\n",
    "print(grads['W2'].shape) # 2 層目の重みの勾配\n",
    "print(grads['b1'].shape) # 2 層目のバイアスの勾配"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 4.5.2 ミニバッチ学習の実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "# データの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "\n",
    "train_loss_list = []\n",
    "\n",
    "# ハイパーパラメータ\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # ミニバッチの取得\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    # 勾配の計算\n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    # grad = network.gradient(x_batch, t_batch)\n",
    "\n",
    "    # パラメータの更新\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "\n",
    "    # 学習経過の記録\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "\n",
    "# グラフの描画\n",
    "x = np.arange(len(train_loss_list))\n",
    "plt.plot(x, train_loss_list, label='train loss')\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 4.5.3 テストデータで評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "# データの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "# 1 エポックあたりの繰り返し数\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # ミニバッチの取得\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 勾配の計算\n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    # grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    # パラメータの更新\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    # 1 エポックごとに認識精度を計算\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))\n",
    "\n",
    "# グラフの描画\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, label='train acc')\n",
    "plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}